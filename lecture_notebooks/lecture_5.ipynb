{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH 651: Advanced Numerical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5sMJY4v8Tol"
   },
   "source": [
    "### Topics\n",
    "\n",
    "* Inner-product spaces\n",
    "* Hilbert spaces\n",
    "* Projections onto subspaces\n",
    "\n",
    "#### Textbook references\n",
    "\n",
    "Sections 2.1, 2.2, 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner-Product Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_DEFINITION:_** Given a space $V$, a **bilinear form** is a mapping $b : V \\times V \\to \\mathbb{R}$ such that\n",
    ">\n",
    "> * $w \\mapsto b(v, w)$ is linear for all $v \\in V$, and\n",
    "> * $v \\mapsto b(v, w)$ is linear for all $w \\in V$\n",
    ">\n",
    "> i.e.\n",
    ">\n",
    "> * $b(u, av + w) = a b(u, v) + b(u, w)$, and\n",
    "> * $b(a u + v, w) = a b(u, v) + b(v, w)$\n",
    ">\n",
    "> for all $u, v, w \\in V$ and $a \\in \\mathbb{R}$.\n",
    ">\n",
    "> $b(\\cdot, \\cdot)$ is **symmetric** if\n",
    ">\n",
    "> * $b(u, v) = b(v, u)$ for all $u, v \\in V$\n",
    ">\n",
    "> An **inner product**, here denoted $(\\cdot, \\cdot)$, is a bilinear form satisfying\n",
    ">\n",
    "> * $(v, v) > 0$ for all $0 \\neq v \\in V$\n",
    ">     * (this property is called **positive definiteness**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_EXAMPLES:_** \n",
    "> \n",
    "> * $\\mathbb{R}^n$ with the usual Euclidean inner product, $$ (x, y) = \\sum_{i=1}^n x_i y_i $$\n",
    "> * $L^2(\\Omega)$ with the $L^2$ inner product $$ (u, v)_{L^2(\\Omega)} = \\int_\\Omega u(x) v(x) \\, dx $$\n",
    "> * $W^k_2(\\Omega)$ with inner product $$ (u, v)_k = \\sum_{|\\alpha| \\leq k} (D^\\alpha u, D^\\alpha v)_{L^2(\\Omega)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTATION:_** The spaces $W^k_2(\\Omega)$ will be denoted $H^k(\\Omega)$. The reason for the letter $H$ will soon be clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_THEOREM (Cauchy-Schwarz Inequality):_**\n",
    "> Given an inner-product space $V$,\n",
    "> $$\n",
    ">  | (u, v) | \\leq (u, u)^{1/2} (v, v)^{1/2}\n",
    "> $$\n",
    "> Equality holds iff $u$ and $v$ are linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Proof._**\n",
    "> For any $t \\in \\mathbb{R}$\n",
    "> $$\n",
    ">  0 \\leq (u - tv, u - tv) = (u, u) - 2t(u, v) + t^2 (v, v).\n",
    "> $$\n",
    ">\n",
    "> If either $u$ or $v$ is zero, then both sides of the inequality are zero.\n",
    "> So, we may assume WLOG that $u$ and $v$ are both nonzero, and in particular, $(v, v) > 0$.\n",
    ">\n",
    "> Let $t = (u, v) / (v, v)$ and insert this into the above, obtaining\n",
    "> $$\n",
    ">  0 \\leq (u,u) - 2 \\frac{(u,v)^2}{(v,v)} + \\frac{(u,v)^2}{(v,v)^2}(v,v) = (u,u) - (u,v)^2 / (v,v)\n",
    "> $$\n",
    "> Rearranging, we obtain\n",
    "> $$\n",
    ">  (u, v)^2 \\leq (u, u) (v, v),\n",
    "> $$\n",
    "> proving the inequality.\n",
    ">\n",
    "> It remains to show that equality holds iff $u$ and $v$ are linearly dependent.\n",
    ">\n",
    "> If $u$ and $v$ are linearly dependent (and nonzero) then $u = t v$ for some $t \\in \\mathbb{R}$.\n",
    "> Then, we have $(u - tv, u - tv) = 0$, and the argument above proves the equality.\n",
    ">\n",
    "> Finally, suppose that the equality holds.\n",
    "> Following the argument above in reverse, this shows that\n",
    "> $$ (u - tv, u - tv) = 0 $$\n",
    "> which by definition of an inner product means that $u - tv = 0$, and so $u = tv$, completing the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_DEFINITION:_** The **norm** $\\| u \\|$ **induced by the inner product** $(\\cdot, \\cdot)$ is defined by\n",
    "> $$ \\| u \\| = \\sqrt{(u, u)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_PROPOSITION:_** The definition of $\\| u \\|$ really defines a norm, i.e. satisfies the properties\n",
    ">\n",
    "> * $\\| u \\| \\geq 0$ for all $u \\in V$\n",
    "> * $\\| u \\| = 0$ iff $ u = 0$\n",
    "> * $\\| a u \\| = |a| \\| u \\|$ for all $a \\in \\mathbb{R}$\n",
    "> * $\\| u + v \\| \\leq \\|u \\| + \\| v \\|$ (triangle inequality)\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Proof._** \n",
    "> The two properties follow immediately from positive definiteness of the inner product.\n",
    "> We also see immediately by bilinearity,\n",
    "> $$ (a u, a u)^{1/2} = ( a^2 (u, u) )^{1/2} = |a| \\| u \\| $$\n",
    "> It remains to prove the triangle inequality.\n",
    ">\n",
    "> $$\n",
    "> \\begin{aligned}\n",
    ">  \\| u + v \\|^2\n",
    ">     &= (u+v, u+v) \\\\\n",
    ">     &= (u,u) + 2(u,v) + (v,v) \\\\\n",
    ">     &\\leq (u,u) + 2(u,u)^{1/2} (v,v)^{1/2} + (v,v) \\\\\n",
    ">     &= ( \\| u \\| + \\| v \\| )^2\n",
    "> \\end{aligned}\n",
    "> $$\n",
    "> Taking the square root of both sides proves the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hilbert Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_DEFINITION:_** An inner-product space $V$ is a **Hilbert space** if it is complete with respect to the norm induced by the inner product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the examples of inner product spaces shown above are also examples of Hilbert spaces.\n",
    "\n",
    "The norm induced by the inner product $(\\cdot, \\cdot)_k$ defined on $W^k_2(\\Omega) = H^k(\\Omega)$ is the **same** as the Sobolev norm $\\| \\cdot \\|_{W^k_2(\\Omega)}$ considered previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_DEFINITION:_** A closed linear space $S \\subseteq H$ of a Hilbert space $H$ is called a **subspace**. $S$ is also a Hilbert space. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of subspaces:\n",
    "\n",
    "* $H$ and $\\{ 0 \\}$ are the extreme examples.\n",
    "* Let $T : H \\to K$ be a continuous linear map from $H$ into some other linear space $K$. Then the kernel of $T$ is a subspace.\n",
    "* Let $x \\in H$ and define $x^\\perp$ denote the set of all $v \\in H$ **orthogonal to** $x$, i.e.\n",
    "   $$ x^\\perp = \\{ v \\in H : (v, x) = 0 \\} $$\n",
    "   Proof. Let $L_x : H \\to \\mathbb{R}$ be defined by $L_x(v) = (v, x)$. Then $x^\\perp = \\operatorname{ker}(L_x)$. If $L_x$ is continuous, then this result follows from the previous example. To see, this\n",
    "   $$ | L_x(v) | = | (v, x) | \\leq \\| x \\| \\| v \\| $$\n",
    "   and the result holds.\n",
    "* For any subset $M \\subseteq H$, then $M^\\perp = \\{ v \\in H : (v, x) = 0 \\text{ for all } x \\in M \\}$ is a subspace of $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_PROPOSITION:_** Let $H$ be a Hilbert space.\n",
    "> 1. For any subsets $M, N$ then $M \\subseteq N$ implies that $N^\\perp \\subseteq M^\\perp$\n",
    "> 2. For any subset $M$ with $0 \\in M$ then $M \\cap M^\\perp = \\{ 0 \\}$\n",
    "> 3. $\\{ 0 \\}^\\perp = H$\n",
    "> 4. $H^\\perp = \\{ 0 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Proof._** \n",
    ">\n",
    "> 1. Suppose $M \\subseteq N$. Let $v \\in N^\\perp$. Take any $x \\in M$. Then, $x \\in N$ since $M \\subseteq N$. So $(x, v) = 0$. This implies that $v \\in M^\\perp$, so $N^\\perp \\subseteq M^\\perp$.\n",
    "> 2. Suppose that $0 \\in M$. Let $v \\in M \\cap M^\\perp$. Then, $(v, v) = 0$ and so $v = 0$, hence $M \\cap M^\\perp = \\{ 0 \\}$.\n",
    "> 3. Let $v \\in H$ be arbitrary. Then $(v, 0) = 0$, so $v \\in 0^\\perp$.\n",
    "> 4. Let $v \\in H^\\perp$. Then, since $v$ is also in $H$, we have $(v, v) = 0$, and $v = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_THEOREM (Parallelogram Law):_**\n",
    "> $$ \\| v + w \\|^2 + \\| v - w \\|^2 = 2\\left( \\| v \\|^2 + \\| w \\|^2 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Proof._** \n",
    ">\n",
    "> $$ \\begin{aligned}\n",
    ">  (v + w, v + w) + (v - w, v - w)\n",
    ">     &= \\| v \\|^2 + 2(v, w) + \\|w\\|^2 + \\|v\\|^2 - 2(v, w) + \\|w\\|^2.\n",
    "> \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_PROPOSITION._** \n",
    "> Let $M \\subseteq H$ be a subspace.\n",
    "> Let $v \\in H \\setminus M$.\n",
    "> Define\n",
    ">  $$ \\delta = \\inf \\{ \\| v - w \\| : w \\in M \\}. $$\n",
    "> Then, there exists some $w_0 \\in M$ such that\n",
    ">\n",
    "> 1. $\\| v - w_0 \\| = \\delta$, i.e., there exists a closest point in $w_0 \\in M$ to $v$\n",
    "> 2. $(v - w_0) \\in M^\\perp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Proof._** \n",
    ">\n",
    "> We first prove statement 1.\n",
    "> \n",
    "> Let $\\{ w_n \\}$ be a minimizing sequence, i.e. $\\| v - w_n \\| \\to \\delta$.\n",
    "> We show that $\\{ w_n \\}$ is a Cauchy sequence.\n",
    ">\n",
    "> Consider two elements of the sequence, $w_n$ and $w_m$.\n",
    "> By the parallelogram law,\n",
    "> $$\n",
    ">  \\| (w_n - v) + (w_m - v) \\|^2 + \\| (w_n - v) - (w_m - v) \\|^2 = 2( \\| w_n - v \\|^2 + \\| w_m - v \\|^2 ).\n",
    "> $$\n",
    "> Rearranging,\n",
    "> $$ \\begin{aligned}\n",
    ">  \\| (w_n - v) - (w_m - v) \\|^2\n",
    ">     &= \\| w_n - w_m \\|^2 \\\\\n",
    ">     &= 2( \\| w_n - v \\|^2 + \\| w_m - v \\|^2 ) - \\| (w_n - v) + (w_m - v) \\|^2 \\\\\n",
    ">     &= 2( \\| w_n - v \\|^2 + \\| w_m - v \\|^2 ) - 4 \\| \\tfrac{1}{2} (w_n + w_m) - v \\|^2 \\\\\n",
    "> \\end{aligned} $$\n",
    "> Since $M$ is a linear space, we have $\\frac{1}{2}(w_n + w_m) \\in M$, and so $\\| \\tfrac{1}{2} (w_n + w_m) - v \\| \\geq \\delta$.\n",
    "> Therefore,\n",
    "> $$\n",
    ">  2( \\| w_n - v \\|^2 + \\| w_m - v \\|^2 ) - 4 \\| \\tfrac{1}{2} (w_n + w_m) - v \\|^2\n",
    ">     \\leq 2 ( \\| w_n - v \\|^2 + \\| w_m - v \\|^2 ) - 4 \\delta^2.\n",
    "> $$\n",
    "> As $n, m \\to \\infty$ we have $\\| w_n - v \\|, \\| w_m - v \\| \\to \\delta$, and so\n",
    "> $$ 2 ( \\| w_n - v \\|^2 + \\| w_m - v \\|^2 ) - 4 \\delta^2 \\to 0 $$\n",
    "> proving that the sequence is Cauchy.\n",
    ">\n",
    "> So, $w_n$ converges to some limit $w_0 \\in H$.\n",
    "> Since $M$ is a subspace, it is closed, so $w_0 \\in M$, proving statement 1.\n",
    ">\n",
    "> Now, we prove statement 2.\n",
    ">\n",
    "> Let $z = w_0 - v$, so that $\\| z \\| = \\delta$. We want to show that $z \\in M^\\perp$.\n",
    "> Let $w \\in M$ be arbitrary.\n",
    "> Since $w_0$ minimizes the distance to $v$, the function\n",
    "> $$ \\| v - (w_0 + t w) \\|^2 $$\n",
    "> has a minimum at $t = 0$ (since $w_0 + t w \\in M$ for $t \\in \\mathbb{R}$).\n",
    "> Letting $z = v - w_0$, we see\n",
    "> $$ \\| v - (w_0 + t w) \\|^2 = \\| z - tw \\|^2 = (z - tw, z - tw) = (z, z) - 2t(z, w) + t^2 (w, w) $$\n",
    "> Since this function has a minimum at $t = 0$, we see that its derivative must vanish at $t = 0$, so\n",
    "> $$ 0 = \\frac{d}{dt} \\| v - (w_0 + tw) \\| |_{t=0} = - 2 (z, w) $$\n",
    "> and therefore $(z, w) = 0$, proving the proposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, let $M \\subseteq H$ be a subspace.\n",
    "Then, for any $v \\in H$, we have the decomposition\n",
    "$$ v = w_0 + w_1$$\n",
    "where $w_0 \\in M$ and $w_1 = v - w_0 \\in M^\\perp$.\n",
    "\n",
    "This decomposition is unique.\n",
    "To see this, let $z_0 + z_1$ be another such decomposition.\n",
    "Then,\n",
    "$$\n",
    "    0 = (w_0 - z_0) + (w_1 - z_1)\n",
    "$$\n",
    "and so\n",
    "$$\n",
    "    M \\ni w_0 - z_0 = z_1 - w_1 \\in M^\\perp\n",
    "$$\n",
    "and since $M \\cap M^\\perp = \\{ 0 \\}$ it follows that $w_0 = z_0$ and $w_1 = z_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore define the orthogonal projections $P_M$ and $P_M^\\perp$ by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    P_M &: v \\mapsto w_0 \\\\\n",
    "    P_M^\\perp &: v \\mapsto w_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $P_M^\\perp = P_{M^\\perp}$. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of writing this is:\n",
    "\n",
    "> $H = M \\oplus M^\\perp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_DEFINITION:_** A linear operator $P : V \\to V$ is a **projection** if $P^2 = P$, i.e. $P ( P z) = P z$ for all $z \\in V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_M$ and $P_M^\\perp$ are projections. Why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "title": "Lecture 5",
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
